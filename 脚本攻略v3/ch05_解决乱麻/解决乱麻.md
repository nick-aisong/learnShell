解决乱麻
========
| 目录                           | 主要命令             |
| ------------------------------ | -------------------- |
| Web页面下载 |          |
| 以纯文本形式下载页面 |          |
| cURL入门 |          |
| 从命令行访问未读的Gmail邮件 |          |
| 解析网站数据 |          |
| 图片爬取器及下载工具 |          |
| 网页相册生成器 |          |
| Twitter命令行客户端 |          |
| 通过Web服务器查询单词含义 |          |
| 查找网站中的无效链接 |          |
| 跟踪网站变动 |          |
| 发送Web页面并读取响应 |          |
| 从Internet下载视频 |          |
| 使用OTS汇总文本 |          |
| 在命令行中翻译文本 |          |

#### Web页面下载

wget是一个用于文件下载的命令行工具，选项繁多且用法灵活

```shell
# 用wget可以下载Web页面或远程文件：
$ wget URL 
# 例如：
$ wget knopper.net
--2016-11-02 21:41:23-- http://knopper.net/
Resolving knopper.net... 85.214.68.145
Connecting to knopper.net|85.214.68.145|:80...
connected.
HTTP request sent, awaiting response... 200 OK
Length: 6899 (6.7K) [text/html]
Saving to: "index.html.1"
100% [=============================>]45.5K=0.1s
2016-11-02 21:41:23 (45.5 KB/s) - "index.html.1" saved
[6899/6899] 

# 可以指定从多个URL处进行下载：
$ wget URL1 URL2 URL3 .. 
```

工作原理

下载的文件名默认和URL中的文件名会保持一致，下载日志和进度被写入stdout

```shell
# 你可以通过选项-O指定输出文件名。如果存在同名文件，那么该文件会被下载文件所取代：
$ wget http://www.knopper.net -O knopper.html 

# 也可以用选项-o指定一个日志文件，这样日志信息就不会被打印到stdout了
$ wget ftp://ftp.example.com/somefile.img -O dloaded_file.img -o log
# 运行该命令，屏幕上不会出现任何内容
# 日志或进度信息都被写入文件log，下载文件为dloaded_file.img

# 由于不稳定的互联网连接，下载有可能被迫中断。选项-t可以指定在放弃下载之前尝试多少次：
$ wget -t 5 URL
# 将-t选项的值设为0会强制wget不断地进行重试：
$ wget -t 0 URL 
```

补充内容

```shell
# 1. 下载限速

# 当下载带宽有限，却又有多个应用程序共享网络连接时，下载大文件会榨干所有的带宽，严重阻滞其他进程（可能是交互式用户）
# 选项--limit-rate可以限定下载任务能够占有的最大带宽，从而保证其他应用程序能够公平地访问Internet:
$ wget --limit-rate 20k http://example.com/file.iso 
# 在命令中可以用k（千字节）和m（兆字节）指定速度限制

# 选项--quota或-Q可以指定最大下载配额（quota）
# 配额一旦用尽，下载随之停止。在下载多个文件时，对于存储空间有限的系统，限制总下载量是有必要的：
$ wget -Q 100m http://example.com/file1 http://example.com/file2 

# 2. 断点续传
# 如果wget在下载完成之前被中断，可以利用选项-c从断点开始继续下载：
$ wget -c URL 

# 3. 复制整个网站（镜像）
# wget像爬虫一样以递归的方式遍历网页上所有的URL链接，并逐个下载
# 要实现这种操作，可以使用选项--mirror：
$ wget --mirror --convert-links exampledomain.com
# 或者
$ wget -r -N -l -k DEPTH URL 
# 选项-l指定页面层级（深度）。这意味着wget只会向下遍历指定层数的页面
# 该选项要与-r（recursive，递归选项）一同使用
# 另外，-N表示使用文件的时间戳。URL表示欲下载的网站起始地址
# -k或--convert-links指示wget将页面的链接地址转换为本地地址

# 4. 访问需要认证的HTTP或FTP页面
# 一些网站需要HTTP或FTP认证，可以用--user和--password提供认证信息：
$ wget --user username --password pass URL 
# 也可以不在命令行中指定密码，而是在网页上手动输入密码，这就需要将--password改为--ask-password
```

#### 以纯文本形式下载页面

















#### cURL入门

















#### 从命令行访问未读的Gmail邮件

















#### 解析网站数据

















#### 图片爬取器及下载工具













#### 网页相册生成器

















#### Twitter命令行客户端

















#### 通过Web服务器查询单词含义















#### 查找网站中的无效链接















#### 跟踪网站变动















#### 发送Web页面并读取响应













#### 从Internet下载视频

















#### 使用OTS汇总文本























#### 在命令行中翻译文本



