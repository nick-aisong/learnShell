解决乱麻
========
| 目录                           | 主要命令             |
| ------------------------------ | -------------------- |
| Web页面下载 |          |
| 以纯文本形式下载页面 |          |
| cURL入门 |          |
| 从命令行访问未读的Gmail邮件 |          |
| 解析网站数据 |          |
| 图片爬取器及下载工具 |          |
| 网页相册生成器 |          |
| Twitter命令行客户端 |          |
| 通过Web服务器查询单词含义 |          |
| 查找网站中的无效链接 |          |
| 跟踪网站变动 |          |
| 发送Web页面并读取响应 |          |
| 从Internet下载视频 |          |
| 使用OTS汇总文本 |          |
| 在命令行中翻译文本 |          |

#### Web页面下载

wget是一个用于文件下载的命令行工具，选项繁多且用法灵活

```shell
# 用wget可以下载Web页面或远程文件：
$ wget URL 
# 例如：
$ wget knopper.net
--2016-11-02 21:41:23-- http://knopper.net/
Resolving knopper.net... 85.214.68.145
Connecting to knopper.net|85.214.68.145|:80...
connected.
HTTP request sent, awaiting response... 200 OK
Length: 6899 (6.7K) [text/html]
Saving to: "index.html.1"
100% [=============================>]45.5K=0.1s
2016-11-02 21:41:23 (45.5 KB/s) - "index.html.1" saved
[6899/6899] 

# 可以指定从多个URL处进行下载：
$ wget URL1 URL2 URL3 .. 
```

工作原理

下载的文件名默认和URL中的文件名会保持一致，下载日志和进度被写入stdout

```shell
# 你可以通过选项-O指定输出文件名。如果存在同名文件，那么该文件会被下载文件所取代：
$ wget http://www.knopper.net -O knopper.html 

# 也可以用选项-o指定一个日志文件，这样日志信息就不会被打印到stdout了
$ wget ftp://ftp.example.com/somefile.img -O dloaded_file.img -o log
# 运行该命令，屏幕上不会出现任何内容
# 日志或进度信息都被写入文件log，下载文件为dloaded_file.img

# 由于不稳定的互联网连接，下载有可能被迫中断。选项-t可以指定在放弃下载之前尝试多少次：
$ wget -t 5 URL
# 将-t选项的值设为0会强制wget不断地进行重试：
$ wget -t 0 URL 
```

补充内容

```shell
# 1. 下载限速

# 当下载带宽有限，却又有多个应用程序共享网络连接时，下载大文件会榨干所有的带宽，严重阻滞其他进程（可能是交互式用户）
# 选项--limit-rate可以限定下载任务能够占有的最大带宽，从而保证其他应用程序能够公平地访问Internet:
$ wget --limit-rate 20k http://example.com/file.iso 
# 在命令中可以用k（千字节）和m（兆字节）指定速度限制

# 选项--quota或-Q可以指定最大下载配额（quota）
# 配额一旦用尽，下载随之停止。在下载多个文件时，对于存储空间有限的系统，限制总下载量是有必要的：
$ wget -Q 100m http://example.com/file1 http://example.com/file2 

# 2. 断点续传
# 如果wget在下载完成之前被中断，可以利用选项-c从断点开始继续下载：
$ wget -c URL 

# 3. 复制整个网站（镜像）
# wget像爬虫一样以递归的方式遍历网页上所有的URL链接，并逐个下载
# 要实现这种操作，可以使用选项--mirror：
$ wget --mirror --convert-links exampledomain.com
# 或者
$ wget -r -N -l -k DEPTH URL 
# 选项-l指定页面层级（深度）。这意味着wget只会向下遍历指定层数的页面
# 该选项要与-r（recursive，递归选项）一同使用
# 另外，-N表示使用文件的时间戳。URL表示欲下载的网站起始地址
# -k或--convert-links指示wget将页面的链接地址转换为本地地址

# 4. 访问需要认证的HTTP或FTP页面
# 一些网站需要HTTP或FTP认证，可以用--user和--password提供认证信息：
$ wget --user username --password pass URL 
# 也可以不在命令行中指定密码，而是在网页上手动输入密码，这就需要将--password改为--ask-password
```

#### 以纯文本形式下载页面

Web页面其实就是包含HTML标签、JavaScript和CSS的文本文件。HTML标签定义了页面内
容，如果要解析页面来查找特定的内容，这时bash就能派上用场了。可以用浏览器查看HTML文
件格式是否正确，也可以用之前讲过的工具对其进行处理

解析文本文件要比解析HTML数据来得容易，因为不用再去剥离HTML标签。Lynx是一款基
于命令行的Web浏览器，能够以纯文本形式下载Web网页



lynx命令默认并没有安装在各种发行版中，不过可以通过包管理器来获取：

```shell
$ sudo yum install lynx
# 或者
apt-get install lynx 
```

```shell
# 选项-dump能够以纯ASCII编码的形式下载Web页面
# 下面的命令可以将下载到的页面保存到文件中：
$ lynx URL -dump > webpage_as_text.txt 

# 这个命令会将页面中所有的超链接（<a href="link">）作为文本文件的页脚
# 单独放置在标题为References的区域
# 这样我们就可以使用正则表达式专门解析链接了。例如：
$ lynx -dump http://google.com > plain_text_page.txt 

# 你可以用cat命令查看纯文本形式的网页：
$ cat plain_text_page.txt
 Search [1]Images [2]Maps [3]Play [4]YouTube [5]News [6]Gmail
 [7]Drive
 [8]More »
 [9]Web History | [10]Settings | [11]Sign in
 [12]St. Patrick's Day 2017 
 _________________________________________________________
 Google Search I'm Feeling Lucky [13]Advanced search
 [14]Language tools
 [15]Advertising Programs [16]Business Solutions [17]+Google
 [18]About Google
 © 2017 - [19]Privacy - [20]Terms
References
... 
```

#### cURL入门

cURL可以使用HTTP、HTTPS、FTP协议在客户端与服务器之间传递数据。它支持POST、
cookie、认证、从指定偏移处下载部分文件、参照页（referer）、用户代理字符串、扩展头部、限
速、文件大小限制、进度条等特性。cURL可用于网站维护、数据检索以及服务器配置核对

和wget不同，并非所有的Linux发行版中都安装了cURL，你得使用包管理器自行安装

cURL默认会将下载文件输出到stdout，将进度信息输出到stderr。如果不想显示进度信
息，可以使用--silent选项



curl命令的用途广泛，其功能包括下载、发送各种HTTP请求以及指定HTTP头部

```shell
# 使用下列命令将下载的文件输出到stdout：
$ curl URL 

# 选项-O指明将下载数据写入文件，采用从URL中解析出的文件名
# 注意，其中的URL必须是完整的，不能仅是站点的域名：
$ curl www.knopper.net/index.htm --silent -O 

# 选项-o可以指定输出文件名
# 如果使用了该选项，只需要写明站点的域名就可以下载其主页了：
$ curl www.knopper.net -o knoppix_index.html
% Total % Received % Xferd Avg Speed Time Time Time
Current
Dload Upload Total Spent Left Speed
100 6889 100 6889 0 0 10902 0 --:-- --:-- --:-- 26033 

# 选项--silent可以让curl命令不显示进度信息：
$ curl URL --silent 

# 如果需要在下载过程中显示形如#的进度条，可以使用选项--progress：
$ curl http://knopper.net -o index.html --progress
################################## 100.0%
```

补充内容

1. 断点续传

```shell
# cURL能够从特定的文件偏移处继续下载
# 如果你每天有流量限制，但又要下载大文件时，这个功能非常有用
$ curl URL/file -C offset 

# 偏移量是以字节为单位的整数
# 如果只是想断点续传，那么cURL不需要指定准确的字节偏移
# 要是你希望cURL推断出正确的续传位置，请使用选项-C -，就像这样：
$ curl -C - URL 
# cURL会自动计算出应该从哪里开始续传
```

2. 用cURL设置参照页字符串

一些动态页面会在返回HTML数据前检测参照页字符串。例如，如果用户是通过Google搜索
来到了当前页面，那么页面上就可以显示一个Google的logo；如果用户是通过手动输入URL来到
当前页面，则显示其他内容

Web开发人员可以根据条件作出判断：如果参照页是www.google.com，那么就返回一个Google
页面，否则返回其他页面

```shell
# 可以用curl命令的 --referer选项指定参照页字符串：
$ curl --referer Referer_URL target_URL
# 例如：
$ curl --referer http://google.com http://knopper.org 
```

3. 用cURL设置cookie 

我们可以用curl来指定并存储HTTP操作过程中使用到的cookie

```shell
# 选项--cookie COOKIE_IDENTIFER可以指定提供哪些cookie
# cookies需要以name=value的形式来给出
# 多个cookie之间使用分号分隔：
$ curl http://example.com --cookie "user=username;pass=hack"
# 选项--cookie-jar可以将cookie另存为文件：
$ curl URL --cookie-jar cookie_file 
```

4. 用cURL设置用户代理字符串

如果不指定用户代理（user agent），一些需要检验用户代理的页面就无法显示。例如，有些
旧网站只能在Internet Explorer（IE）下正常工作。如果使用其他浏览器，则会提示只能用IE访问。
这是因为这些网站检查了用户代理。你可以用curl来设置用户代理

```shell
# cURL的选项--user-agent或-A用于设置用户代理：
$ curl URL --user-agent "Mozilla/5.0"

# cURL也能够发送其他HTTP头部信息。使用-H "Header"传递多个头部信息：
$ curl -H "Host: www.knopper.net" -H "Accept-language: en" URL 
```

浏览器和爬虫使用的用户代理字符串各不相同。你可以在这里找到其中的一部分：http://www.useragentstring.com/pages/useragentstring.php

5. 限定cURL可占用的带宽

```shell
# 如果多个用户共享带宽有限，我们可以用--limit-rate限制cURL的下载速度：
$ curl URL --limit-rate 20k
# 在命令中用k（千字节）和m（兆字节）指定下载速度限制
```

6. 指定最大下载量

```shell
# 可以用--max-filesize选项指定可下载的最大文件大小：
$ curl URL --max-filesize bytes 
```

如果文件大小超出限制，命令返回一个非0的退出码。如果文件下载成功，则返回0

7. 用cURL进行认证

```shell
# 可以用curl的选项-u完成HTTP或FTP认证
# 使用-u username:password来指定用户名和密码：
$ curl -u user:pass http://test_auth.com

# 如果你喜欢经提示后输入密码，只需要使用用户名即可：
$ curl -u user http://test_auth.com 
```

8. 只打印响应头部信息（不包括数据部分）

只检查头部信息就足以完成很多检查或统计。例如，如果要检查某个页面是否能够打开，并
不需要下载整个页面内容。只读取HTTP响应头部就足够了

检查HTTP头部的另一种用法就是通过检查其中的Content-Length字段来得知文件的大
小，或是检查Last-Modified字段，在下载之前了解文件是否比当前版本更新

```shell
# 选项-I或--head可以只打印HTTP头部信息，无须下载远程文件：
$ curl -I http://knopper.net
HTTP/1.1 200 OK
Date: Tue, 08 Nov 2016 17:15:21 GMT
Server: Apache
Last-Modified: Wed, 26 Oct 2016 23:29:56 GMT
ETag: "1d3c8-1af3-b10500"
Accept-Ranges: bytes
Content-Length: 6899
Content-Type: text/html; charset=ISO-8859-1 
```

#### 从命令行访问未读的Gmail邮件

Gmail（https://mail.google.com）是Google所提供的一项被广泛使用的免费电子邮件服务。你
可以通过浏览器或经过认证的RSS feed来读取个人邮件。我们解析RSS feed来获取发件人姓名和
邮件主题。这种方法无需打开浏览器就能够快速地查看未读邮件

```shell
# 来看下面这个脚本文件，它的作用是通过解析Gmail的RSS feed来显示未读的邮件：

#!/bin/bash
# 用途: Gmail邮件读取工具
username='PUT_USERNAME_HERE'
password='PUT_PASSWORD_HERE'
SHOW_COUNT=5 # 需要显示的未读邮件数量
echo
curl -u $username:$password --silent \
  "https://mail.google.com/mail/feed/atom" | \
  tr -d '\n' | sed 's:</entry>:\n:g' |\
  sed -n
's/.*<title>\(.*\)<\/title.*<author><name>\([^<]*\)<\/name><email>
 \([^<]*\).*/From: \2 [\3] \nSubject: \1\n/p' | \
head -n $(( $SHOW_COUNT * 3 )) 

# 输出如下:
$ ./fetch_gmail.sh
From: SLYNUX [ slynux@slynux.com ]
Subject: Book release - 2
From: SLYNUX [ slynux@slynux.com ]
Subject: Book release - 1
.
... 5 entries 
```

如果你的Gmail账户开启了双重身份认证，那就必须为此脚本生成一个新的
密钥并使用。你的普通密码就不能再用了

工作原理

这个脚本使用cURL来下载RSS feed。你可以登录Gmail账户，在https://mail.google.com/mail/
feed/atom查看下载到的数据格式

```shell
# cURL使用-u user: pass所提供的用户认证信息来读入RSS feed
# 如果只用了-u user，cURL在运行时会要求输入密码

# tr -d '\n'移除了所有的换行符
# sed 's:</entry>:\n:g'将每一处</entry>替换成换行符，以保证每一条邮件项独立成行，以便逐行解析邮件

# 该脚本接下来的部分作为sed的单个表达式执行，用于提取相关字段：
sed 's/.*<title>\(.*\)<\/title.*<author><name>\([^<]*\)<\/name><email>
\([^<]*\).*/Author: \2 [\3] \nSubject: \1\n/' 

# 脚本用<title>\(.*\)<\/title匹配邮件标题，<author><name>\([^<]*\)<\/ name>匹配发件人姓名，<email>\([^<]*\)匹配发件人电子邮件地址
# sed利用反向引用，将邮件的作者（author）、标题（title）和主题（subject）以易读的形式显示出来：
Author: \2 [\3] \nSubject: \1\n 
# \1对应于第一处匹配（邮件标题），\2对应于第二处匹配（发件人姓名），以此类推
# SHOW_COUNT=5用来设置需要在终端中显示的未读邮件数量
# head用来显示SHOW_COUNT*3行文本。SHOW_COUNT乘以3是因为每一封未读邮件的相关信息需要占用3行
```


#### 解析网站数据

lynx、sed和awk都可以用来挖掘网站数据。在第4章有关grep的攻略中（4.3节），我们见到
过一份演员评级列表。那个列表就是通过解析http://www.ohntorres.net/BoxOfficefemaleList.html
得到的

```shell
# 下面来讲解用于从网站解析演员详细信息的命令：
$ lynx -dump -nolist \
    http://www.johntorres.net/BoxOfficefemaleList.html
	grep -o "Rank-.*" | \
	sed -e 's/ *Rank-\([0-9]*\) *\(.*\)/\1\t\2/' | \
	sort -nk 1 > actresslist.txt 

# 输出如下：
# 由于篇幅有限，故只显示前3位演员的信息
1 Keira Knightley
2 Natalie Portman
3 Monica Bellucci 
```

#### 图片爬取器及下载工具

图片爬取器（image crawler）可以下载Web页面上所有的图片。不用翻遍页面手动保存图片，
我们可以用脚本识别图片并自动下载

```shell
# 下面的bash脚本可以识别并下载Web页面上的图片：

#!/bin/bash
#用途:图片下载工具
#文件名: img_downloader.sh
if [ $# -ne 3 ];
then
  echo "Usage: $0 URL -d DIRECTORY"
  exit -1
fi 
while [ $# -gt 0 ]
do
  case $1 in
    -d) shift; directory=$1; shift ;;
    *) url=$1; shift;;
  esac
done

mkdir -p $directory;
baseurl=$(echo $url | egrep -o "https?://[a-z.\-]+")

echo Downloading $url
curl -s $url | egrep -o "<img[^>]*src=[^>]*>" | \
  sed 's/<img[^>]*src=\"\([^"]*\).*/\1/g' | \
  sed "s,^/,$baseurl/," > /tmp/$$.list
  
cd $directory;

while read filename;
do
  echo Downloading $filename
  curl -s -O "$filename" --silent
done < /tmp/$$.list

# 使用方法：
$ url=https://commons.wikimedia.org/wiki/Main_Page
$ ./img_downloader.sh $url -d images 
```

工作原理

```shell
# 图片下载器脚本首先解析HTML页面，除去<img>之外的所有标签，然后从<img>标签中解析出src="URL"并将图片下载到指定的目录中
# 这个脚本接受一个Web页面的URL和用于存放图片的目录作为命令行参数
# [ $# -ne 3 ]用于检查脚本参数数量是否为3个
# 如果不是，脚本会退出运行并显示使用说明
# 如果参数没有问题，就解析URL和目标目录：
while [ -n "$1" ]
do
  case $1 in
    -d) shift; directory=$1; shift ;;
    *) url=${url:-$1}; shift;;
  esac
done 
# while循环会一直处理完所有的参数
# shift用来向左移动参数，这样$2的值就会被赋给 $1，$3的值被赋给 $2，往后以此类推
# 因此通过 $1就可以求值所有的参数
# case语句检查第一个参数（$1）。如果匹配-d，那么下一个参数一定是目录，接着就移动参数并保存目录名。否则的话，就是URL
# 采用这种方法来解析命令行参数的好处在于可以将-d置于命令行中的任意位置：
$ ./img_downloader.sh -d DIR URL
# 或者
$ ./img_downloader.sh URL -d DIR 
# egrep -o "<img src=[^>]*>"只打印带有属性值的<img>标签。[^>]*用来匹配除>之外的所有字符，也就是<img src="image.jpg">
# sed 's/<img src=\"\([^"]*\).*/\1/g'可以从字符串src="url"中提取出url
# 图像文件源路径有两种类型：相对路径和绝对路径。绝对路径包含以http:// 或 https://起始的完整URL，相对路径则以/或图像文件名起始
# 例如http://example.com/image.jpg就是绝对路径，而/image.jpg则是相对路径
# 对于以/起始的相对路径，应该用基址URL（base URL）把它转换为 http://example.com/image.jpg
# 脚本初始化baseurl的方法是使用下列命令从初始URL中提取基址部分：
baseurl=$(echo $url | egrep -o "https?://[a-z.\-]+") 
# 上述sed命令的输出通过管道传入另一个sed命令，后者使用baseurl替换掉起始的/（leading /），其结果被保存在以脚本PID为名的文件中（/tmp/$$.list）：
sed "s,^/,$baseurl/," > /tmp/$$.list 
# 最后的while循环用来逐行迭代图片的URL列表并使用curl下载图像文件
# curl的--silent选项可避免在屏幕上出现下载进度信息
```

#### 网页相册生成器

















#### Twitter命令行客户端

















#### 通过Web服务器查询单词含义















#### 查找网站中的无效链接















#### 跟踪网站变动















#### 发送Web页面并读取响应













#### 从Internet下载视频

















#### 使用OTS汇总文本























#### 在命令行中翻译文本



