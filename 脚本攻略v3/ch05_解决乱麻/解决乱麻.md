解决乱麻
========
| 目录                           | 主要命令             |
| ------------------------------ | -------------------- |
| Web页面下载 | wget |
| 以纯文本形式下载页面 | lynx |
| cURL入门 | curl |
| 从命令行访问未读的Gmail邮件 | curl |
| 解析网站数据 | curl |
| 图片爬取器及下载工具 | curl |
| 网页相册生成器 | convert |
| Twitter命令行客户端 |          |
| 通过Web服务器查询单词含义 | curl |
| 查找网站中的无效链接 | lynx、curl |
| 跟踪网站变动 | curl、diff |
| 发送Web页面并读取响应 | curl、wget |
| 从Internet下载视频 | youtube-dl |
| 使用OTS汇总文本 | ots |
| 在命令行中翻译文本 | trans |

#### Web页面下载

wget是一个用于文件下载的命令行工具，选项繁多且用法灵活

```shell
# 用wget可以下载Web页面或远程文件：
$ wget URL 
# 例如：
$ wget knopper.net
--2016-11-02 21:41:23-- http://knopper.net/
Resolving knopper.net... 85.214.68.145
Connecting to knopper.net|85.214.68.145|:80...
connected.
HTTP request sent, awaiting response... 200 OK
Length: 6899 (6.7K) [text/html]
Saving to: "index.html.1"
100% [=============================>]45.5K=0.1s
2016-11-02 21:41:23 (45.5 KB/s) - "index.html.1" saved
[6899/6899] 

# 可以指定从多个URL处进行下载：
$ wget URL1 URL2 URL3 .. 
```

工作原理

下载的文件名默认和URL中的文件名会保持一致，下载日志和进度被写入stdout

```shell
# 你可以通过选项-O指定输出文件名。如果存在同名文件，那么该文件会被下载文件所取代：
$ wget http://www.knopper.net -O knopper.html 

# 也可以用选项-o指定一个日志文件，这样日志信息就不会被打印到stdout了
$ wget ftp://ftp.example.com/somefile.img -O dloaded_file.img -o log
# 运行该命令，屏幕上不会出现任何内容
# 日志或进度信息都被写入文件log，下载文件为dloaded_file.img

# 由于不稳定的互联网连接，下载有可能被迫中断。选项-t可以指定在放弃下载之前尝试多少次：
$ wget -t 5 URL
# 将-t选项的值设为0会强制wget不断地进行重试：
$ wget -t 0 URL 
```

补充内容

```shell
# 1. 下载限速

# 当下载带宽有限，却又有多个应用程序共享网络连接时，下载大文件会榨干所有的带宽，严重阻滞其他进程（可能是交互式用户）
# 选项--limit-rate可以限定下载任务能够占有的最大带宽，从而保证其他应用程序能够公平地访问Internet:
$ wget --limit-rate 20k http://example.com/file.iso 
# 在命令中可以用k（千字节）和m（兆字节）指定速度限制

# 选项--quota或-Q可以指定最大下载配额（quota）
# 配额一旦用尽，下载随之停止。在下载多个文件时，对于存储空间有限的系统，限制总下载量是有必要的：
$ wget -Q 100m http://example.com/file1 http://example.com/file2 

# 2. 断点续传
# 如果wget在下载完成之前被中断，可以利用选项-c从断点开始继续下载：
$ wget -c URL 

# 3. 复制整个网站（镜像）
# wget像爬虫一样以递归的方式遍历网页上所有的URL链接，并逐个下载
# 要实现这种操作，可以使用选项--mirror：
$ wget --mirror --convert-links exampledomain.com
# 或者
$ wget -r -N -l -k DEPTH URL 
# 选项-l指定页面层级（深度）。这意味着wget只会向下遍历指定层数的页面
# 该选项要与-r（recursive，递归选项）一同使用
# 另外，-N表示使用文件的时间戳。URL表示欲下载的网站起始地址
# -k或--convert-links指示wget将页面的链接地址转换为本地地址

# 4. 访问需要认证的HTTP或FTP页面
# 一些网站需要HTTP或FTP认证，可以用--user和--password提供认证信息：
$ wget --user username --password pass URL 
# 也可以不在命令行中指定密码，而是在网页上手动输入密码，这就需要将--password改为--ask-password
```

#### 以纯文本形式下载页面

Web页面其实就是包含HTML标签、JavaScript和CSS的文本文件。HTML标签定义了页面内
容，如果要解析页面来查找特定的内容，这时bash就能派上用场了。可以用浏览器查看HTML文
件格式是否正确，也可以用之前讲过的工具对其进行处理

解析文本文件要比解析HTML数据来得容易，因为不用再去剥离HTML标签。Lynx是一款基
于命令行的Web浏览器，能够以纯文本形式下载Web网页



lynx命令默认并没有安装在各种发行版中，不过可以通过包管理器来获取：

```shell
$ sudo yum install lynx
# 或者
apt-get install lynx 
```

```shell
# 选项-dump能够以纯ASCII编码的形式下载Web页面
# 下面的命令可以将下载到的页面保存到文件中：
$ lynx URL -dump > webpage_as_text.txt 

# 这个命令会将页面中所有的超链接（<a href="link">）作为文本文件的页脚
# 单独放置在标题为References的区域
# 这样我们就可以使用正则表达式专门解析链接了。例如：
$ lynx -dump http://google.com > plain_text_page.txt 

# 你可以用cat命令查看纯文本形式的网页：
$ cat plain_text_page.txt
 Search [1]Images [2]Maps [3]Play [4]YouTube [5]News [6]Gmail
 [7]Drive
 [8]More »
 [9]Web History | [10]Settings | [11]Sign in
 [12]St. Patrick's Day 2017 
 _________________________________________________________
 Google Search I'm Feeling Lucky [13]Advanced search
 [14]Language tools
 [15]Advertising Programs [16]Business Solutions [17]+Google
 [18]About Google
 © 2017 - [19]Privacy - [20]Terms
References
... 
```

#### cURL入门

cURL可以使用HTTP、HTTPS、FTP协议在客户端与服务器之间传递数据。它支持POST、
cookie、认证、从指定偏移处下载部分文件、参照页（referer）、用户代理字符串、扩展头部、限
速、文件大小限制、进度条等特性。cURL可用于网站维护、数据检索以及服务器配置核对

和wget不同，并非所有的Linux发行版中都安装了cURL，你得使用包管理器自行安装

cURL默认会将下载文件输出到stdout，将进度信息输出到stderr。如果不想显示进度信
息，可以使用--silent选项



curl命令的用途广泛，其功能包括下载、发送各种HTTP请求以及指定HTTP头部

```shell
# 使用下列命令将下载的文件输出到stdout：
$ curl URL 

# 选项-O指明将下载数据写入文件，采用从URL中解析出的文件名
# 注意，其中的URL必须是完整的，不能仅是站点的域名：
$ curl www.knopper.net/index.htm --silent -O 

# 选项-o可以指定输出文件名
# 如果使用了该选项，只需要写明站点的域名就可以下载其主页了：
$ curl www.knopper.net -o knoppix_index.html
% Total % Received % Xferd Avg Speed Time Time Time
Current
Dload Upload Total Spent Left Speed
100 6889 100 6889 0 0 10902 0 --:-- --:-- --:-- 26033 

# 选项--silent可以让curl命令不显示进度信息：
$ curl URL --silent 

# 如果需要在下载过程中显示形如#的进度条，可以使用选项--progress：
$ curl http://knopper.net -o index.html --progress
################################## 100.0%
```

补充内容

1. 断点续传

```shell
# cURL能够从特定的文件偏移处继续下载
# 如果你每天有流量限制，但又要下载大文件时，这个功能非常有用
$ curl URL/file -C offset 

# 偏移量是以字节为单位的整数
# 如果只是想断点续传，那么cURL不需要指定准确的字节偏移
# 要是你希望cURL推断出正确的续传位置，请使用选项-C -，就像这样：
$ curl -C - URL 
# cURL会自动计算出应该从哪里开始续传
```

2. 用cURL设置参照页字符串

一些动态页面会在返回HTML数据前检测参照页字符串。例如，如果用户是通过Google搜索
来到了当前页面，那么页面上就可以显示一个Google的logo；如果用户是通过手动输入URL来到
当前页面，则显示其他内容

Web开发人员可以根据条件作出判断：如果参照页是www.google.com，那么就返回一个Google
页面，否则返回其他页面

```shell
# 可以用curl命令的 --referer选项指定参照页字符串：
$ curl --referer Referer_URL target_URL
# 例如：
$ curl --referer http://google.com http://knopper.org 
```

3. 用cURL设置cookie 

我们可以用curl来指定并存储HTTP操作过程中使用到的cookie

```shell
# 选项--cookie COOKIE_IDENTIFER可以指定提供哪些cookie
# cookies需要以name=value的形式来给出
# 多个cookie之间使用分号分隔：
$ curl http://example.com --cookie "user=username;pass=hack"
# 选项--cookie-jar可以将cookie另存为文件：
$ curl URL --cookie-jar cookie_file 
```

4. 用cURL设置用户代理字符串

如果不指定用户代理（user agent），一些需要检验用户代理的页面就无法显示。例如，有些
旧网站只能在Internet Explorer（IE）下正常工作。如果使用其他浏览器，则会提示只能用IE访问。
这是因为这些网站检查了用户代理。你可以用curl来设置用户代理

```shell
# cURL的选项--user-agent或-A用于设置用户代理：
$ curl URL --user-agent "Mozilla/5.0"

# cURL也能够发送其他HTTP头部信息。使用-H "Header"传递多个头部信息：
$ curl -H "Host: www.knopper.net" -H "Accept-language: en" URL 
```

浏览器和爬虫使用的用户代理字符串各不相同。你可以在这里找到其中的一部分：http://www.useragentstring.com/pages/useragentstring.php

5. 限定cURL可占用的带宽

```shell
# 如果多个用户共享带宽有限，我们可以用--limit-rate限制cURL的下载速度：
$ curl URL --limit-rate 20k
# 在命令中用k（千字节）和m（兆字节）指定下载速度限制
```

6. 指定最大下载量

```shell
# 可以用--max-filesize选项指定可下载的最大文件大小：
$ curl URL --max-filesize bytes 
```

如果文件大小超出限制，命令返回一个非0的退出码。如果文件下载成功，则返回0

7. 用cURL进行认证

```shell
# 可以用curl的选项-u完成HTTP或FTP认证
# 使用-u username:password来指定用户名和密码：
$ curl -u user:pass http://test_auth.com

# 如果你喜欢经提示后输入密码，只需要使用用户名即可：
$ curl -u user http://test_auth.com 
```

8. 只打印响应头部信息（不包括数据部分）

只检查头部信息就足以完成很多检查或统计。例如，如果要检查某个页面是否能够打开，并
不需要下载整个页面内容。只读取HTTP响应头部就足够了

检查HTTP头部的另一种用法就是通过检查其中的Content-Length字段来得知文件的大
小，或是检查Last-Modified字段，在下载之前了解文件是否比当前版本更新

```shell
# 选项-I或--head可以只打印HTTP头部信息，无须下载远程文件：
$ curl -I http://knopper.net
HTTP/1.1 200 OK
Date: Tue, 08 Nov 2016 17:15:21 GMT
Server: Apache
Last-Modified: Wed, 26 Oct 2016 23:29:56 GMT
ETag: "1d3c8-1af3-b10500"
Accept-Ranges: bytes
Content-Length: 6899
Content-Type: text/html; charset=ISO-8859-1 
```

#### 从命令行访问未读的Gmail邮件

Gmail（https://mail.google.com）是Google所提供的一项被广泛使用的免费电子邮件服务。你
可以通过浏览器或经过认证的RSS feed来读取个人邮件。我们解析RSS feed来获取发件人姓名和
邮件主题。这种方法无需打开浏览器就能够快速地查看未读邮件

```shell
# 来看下面这个脚本文件，它的作用是通过解析Gmail的RSS feed来显示未读的邮件：

#!/bin/bash
# 用途: Gmail邮件读取工具
username='PUT_USERNAME_HERE'
password='PUT_PASSWORD_HERE'
SHOW_COUNT=5 # 需要显示的未读邮件数量
echo
curl -u $username:$password --silent \
  "https://mail.google.com/mail/feed/atom" | \
  tr -d '\n' | sed 's:</entry>:\n:g' |\
  sed -n
's/.*<title>\(.*\)<\/title.*<author><name>\([^<]*\)<\/name><email>
 \([^<]*\).*/From: \2 [\3] \nSubject: \1\n/p' | \
head -n $(( $SHOW_COUNT * 3 )) 

# 输出如下:
$ ./fetch_gmail.sh
From: SLYNUX [ slynux@slynux.com ]
Subject: Book release - 2
From: SLYNUX [ slynux@slynux.com ]
Subject: Book release - 1
.
... 5 entries 
```

如果你的Gmail账户开启了双重身份认证，那就必须为此脚本生成一个新的
密钥并使用。你的普通密码就不能再用了

工作原理

这个脚本使用cURL来下载RSS feed。你可以登录Gmail账户，在https://mail.google.com/mail/
feed/atom查看下载到的数据格式

```shell
# cURL使用-u user: pass所提供的用户认证信息来读入RSS feed
# 如果只用了-u user，cURL在运行时会要求输入密码

# tr -d '\n'移除了所有的换行符
# sed 's:</entry>:\n:g'将每一处</entry>替换成换行符，以保证每一条邮件项独立成行，以便逐行解析邮件

# 该脚本接下来的部分作为sed的单个表达式执行，用于提取相关字段：
sed 's/.*<title>\(.*\)<\/title.*<author><name>\([^<]*\)<\/name><email>
\([^<]*\).*/Author: \2 [\3] \nSubject: \1\n/' 

# 脚本用<title>\(.*\)<\/title匹配邮件标题，<author><name>\([^<]*\)<\/ name>匹配发件人姓名，<email>\([^<]*\)匹配发件人电子邮件地址
# sed利用反向引用，将邮件的作者（author）、标题（title）和主题（subject）以易读的形式显示出来：
Author: \2 [\3] \nSubject: \1\n 
# \1对应于第一处匹配（邮件标题），\2对应于第二处匹配（发件人姓名），以此类推
# SHOW_COUNT=5用来设置需要在终端中显示的未读邮件数量
# head用来显示SHOW_COUNT*3行文本。SHOW_COUNT乘以3是因为每一封未读邮件的相关信息需要占用3行
```


#### 解析网站数据

lynx、sed和awk都可以用来挖掘网站数据。在第4章有关grep的攻略中（4.3节），我们见到
过一份演员评级列表。那个列表就是通过解析http://www.ohntorres.net/BoxOfficefemaleList.html
得到的

```shell
# 下面来讲解用于从网站解析演员详细信息的命令：
$ lynx -dump -nolist \
    http://www.johntorres.net/BoxOfficefemaleList.html
	grep -o "Rank-.*" | \
	sed -e 's/ *Rank-\([0-9]*\) *\(.*\)/\1\t\2/' | \
	sort -nk 1 > actresslist.txt 

# 输出如下：
# 由于篇幅有限，故只显示前3位演员的信息
1 Keira Knightley
2 Natalie Portman
3 Monica Bellucci 
```

#### 图片爬取器及下载工具

图片爬取器（image crawler）可以下载Web页面上所有的图片。不用翻遍页面手动保存图片，
我们可以用脚本识别图片并自动下载

```shell
# 下面的bash脚本可以识别并下载Web页面上的图片：

#!/bin/bash
#用途:图片下载工具
#文件名: img_downloader.sh
if [ $# -ne 3 ];
then
  echo "Usage: $0 URL -d DIRECTORY"
  exit -1
fi 
while [ $# -gt 0 ]
do
  case $1 in
    -d) shift; directory=$1; shift ;;
    *) url=$1; shift;;
  esac
done

mkdir -p $directory;
baseurl=$(echo $url | egrep -o "https?://[a-z.\-]+")

echo Downloading $url
curl -s $url | egrep -o "<img[^>]*src=[^>]*>" | \
  sed 's/<img[^>]*src=\"\([^"]*\).*/\1/g' | \
  sed "s,^/,$baseurl/," > /tmp/$$.list
  
cd $directory;

while read filename;
do
  echo Downloading $filename
  curl -s -O "$filename" --silent
done < /tmp/$$.list

# 使用方法：
$ url=https://commons.wikimedia.org/wiki/Main_Page
$ ./img_downloader.sh $url -d images 
```

工作原理

```shell
# 图片下载器脚本首先解析HTML页面，除去<img>之外的所有标签，然后从<img>标签中解析出src="URL"并将图片下载到指定的目录中
# 这个脚本接受一个Web页面的URL和用于存放图片的目录作为命令行参数
# [ $# -ne 3 ]用于检查脚本参数数量是否为3个
# 如果不是，脚本会退出运行并显示使用说明
# 如果参数没有问题，就解析URL和目标目录：
while [ -n "$1" ]
do
  case $1 in
    -d) shift; directory=$1; shift ;;
    *) url=${url:-$1}; shift;;
  esac
done 
# while循环会一直处理完所有的参数
# shift用来向左移动参数，这样$2的值就会被赋给 $1，$3的值被赋给 $2，往后以此类推
# 因此通过 $1就可以求值所有的参数
# case语句检查第一个参数（$1）。如果匹配-d，那么下一个参数一定是目录，接着就移动参数并保存目录名。否则的话，就是URL
# 采用这种方法来解析命令行参数的好处在于可以将-d置于命令行中的任意位置：
$ ./img_downloader.sh -d DIR URL
# 或者
$ ./img_downloader.sh URL -d DIR 
# egrep -o "<img src=[^>]*>"只打印带有属性值的<img>标签。[^>]*用来匹配除>之外的所有字符，也就是<img src="image.jpg">
# sed 's/<img src=\"\([^"]*\).*/\1/g'可以从字符串src="url"中提取出url
# 图像文件源路径有两种类型：相对路径和绝对路径。绝对路径包含以http:// 或 https://起始的完整URL，相对路径则以/或图像文件名起始
# 例如http://example.com/image.jpg就是绝对路径，而/image.jpg则是相对路径
# 对于以/起始的相对路径，应该用基址URL（base URL）把它转换为 http://example.com/image.jpg
# 脚本初始化baseurl的方法是使用下列命令从初始URL中提取基址部分：
baseurl=$(echo $url | egrep -o "https?://[a-z.\-]+") 
# 上述sed命令的输出通过管道传入另一个sed命令，后者使用baseurl替换掉起始的/（leading /），其结果被保存在以脚本PID为名的文件中（/tmp/$$.list）：
sed "s,^/,$baseurl/," > /tmp/$$.list 
# 最后的while循环用来逐行迭代图片的URL列表并使用curl下载图像文件
# curl的--silent选项可避免在屏幕上出现下载进度信息
```

#### 网页相册生成器

Web开发人员经常会创建包含全尺寸和缩略图的相册。点击缩略图，就会出现一幅放大的图
片。但如果需要很多图片，每一次都得复制\<img\>标签、调整图片大小来创建缩略图、把调整好
的图片放进缩略图目录。我们可以写一个简单的Bash脚本将这些重复的工作自动化。这样一来，
创建缩略图、将缩略图放入对应的目录、生成<img>标签都可以自动搞定

脚本使用for循环迭代当前目录下的所有图片。这需要借助一些常见的bash工具，如cat和
convert（来自Image Magick软件包）。我们将在index.html中生成一个包含了所有图片的HTML
相册

```shell
# 生成HTML相册页面的bash脚本如下：

#!/bin/bash
# 文件名: generate_album.sh
# 用途: 用当前目录下的图片创建相册
echo "Creating album.."
mkdir -p thumbs
cat <<EOF1 > index.html
<html>
<head>
<style>
body
{
  width:470px;
  margin:auto;
  border: 1px dashed grey;
  padding:10px;
}
img
{
  margin:5px;
  border: 1px solid black;
}
</style>
</head>
<body>
<center><h1> #Album title </h1></center>
<p>
EOF1

for img in *.jpg;
do
  convert "$img" -resize "100x" "thumbs/$img"
  echo "<a href=\"$img\" >" >>index.html 
  echo "<img src=\"thumbs/$img\" title=\"$img\" /></a>" >> index.html
done

cat <<EOF2 >> index.html

</p>
</body>
</html>
EOF2

echo Album generated to index.html 

# 运行脚本：
$ ./generate_album.sh
Creating album..
Album generated to index.html 
```

工作原理

```shell
# 脚本的起始部分用于生成HTML页面的头部
# 接下来，脚本将一直到EOF1的这部分内容（不包括EOF1）重定向到index.html：
cat <<EOF1 > index.html
contents...
EOF1 
# 页面头部包括HTML和CSS样式
# for img in *.jpg;对每一个文件进行迭代并执行相应的操作
# convert "$img" -resize "100x" "thumbs/$img"将创建宽度为100像素的图像缩略图
# 下面的语句会生成所需的<img>标签并将其添加到index.html中：
echo "<a href=\"$img\" >"
echo "<img src=\"thumbs/$img\" title=\"$img\" /></a>" >> index.html 
# 最后再用cat添加HTML页脚，实现方法和添加页面头部一样
```


#### Twitter命令行客户端

Twitter不仅是最流行的微博平台，同时也是最时髦的在线社交媒体。我们可以使用Twitter API从命令行中读取自己的时间线

最近Twitter已经不再允许用户使用普通的HTTP认证（plain HTTP Authentication）登录了，
我们必须使用OAuth进行自身认证（authenticate ourselves）。完整地讲解OAuth超出了本书的范围，
因此我们会利用一个代码库，以便在Bash脚本中可以方便地使用OAuth

(1)  从https://github.com/livibetter/bash-oauth/archive/master.zip处下载bash-oauth库，将其解
压缩到任意目录中
(2)  进入该目录中的bash-oauth-master子目录，以root身份执行make install-all
(3)  进入https://apps.twitter.com/注册新的应用，以便能够使用OAuth
(4)  注册完新的应用之后，进入应用设置，将Access type更改为Read and Write
(5)  进入应用的Details部分，注意两个地方：Consumer Key和Consumer Secret，以便在脚本
中替换相应的部分

```shell
# 下面的bash脚本使用OAuth库读取或发送你的tweet：

#!/bin/bash
# 文件名: twitter.sh
# 用途:twitter客户端基本版

oauth_consumer_key=YOUR_CONSUMER_KEY
oauth_consumer_secret=YOUR_CONSUMER_SECRET

config_file=~/.$oauth_consumer_key-$oauth_consumer_secret-rc

if [[ "$1" != "read" ]] && [[ "$1" != "tweet" ]];
then
  echo -e "Usage: $0 tweet status_message\n OR\n $0 read\n"
  exit -1;
fi

#source /usr/local/bin/TwitterOAuth.sh
source bash-oauth-master/TwitterOAuth.sh
TO_init

if [ ! -e $config_file ]; then
  TO_access_token_helper 
  if (( $? == 0 )); then
  echo oauth_token=${TO_ret[0]} > $config_file
  echo oauth_token_secret=${TO_ret[1]} >> $config_file
  fi
fi

source $config_file

if [[ "$1" = "read" ]];
then
TO_statuses_home_timeline '' 'YOUR_TWEET_NAME' '10'
  echo $TO_ret | sed 's/,"/\n/g' | sed 's/":/~/' | \
    awk -F~ '{} \
     {if ($1 == "text") \
       {txt=$2;} \
      else if ($1 == "screen_name") \
       printf("From: %s\n Tweet: %s\n\n", $2, txt);} \
      {}' | tr '"' ' '
      
elif [[ "$1" = "tweet" ]];
then
  shift
  TO_statuses_update '' "$@"
  echo 'Tweeted :)'
fi 

# 运行脚本：
$./twitter.sh read
Please go to the following link to get the PIN:
https://api.twitter.com/oauth/authorize?
oauth_token=LONG_TOKEN_STRING
PIN: PIN_FROM_WEBSITE
Now you can create, edit and present Slides offline.
- by A Googler
$./twitter.sh tweet "I am reading Packt Shell Scripting Cookbook"
Tweeted :)
$./twitter.sh read | head -2
From: Clif Flynt
Tweet: I am reading Packt Shell Scripting Cookbook 
```

工作原理

```shell
# 首先，使用source命令引入TwitterOAuth.sh库，这样就可以利用其中定义好的函数访问Twitter了。函数TO_init负责初始化库

# 所有的应用在首次使用的时候都需要获取一个OAuth令牌（token）以及令牌密钥（tokensecret）。如果没有得到，则调用库函数TO_access_token_helper。拿到令牌之后，将其保存在config文件中，以后再执行脚本时，只需对该文件执行source命令就可以了

# 库函数TO_statuses_home_timeline可以从Twitter中获取发布的内容。该函数返回的数据是一个JSON格式的长字符串，类似于下列形式：
[{"created_at":"Thu Nov 10 14:45:20 +0000
"016","id":7...9,"id_str":"7...9","text":"Dining... 

# 每条tweet都是以created_at标签作为起始，其中还包含了text和screen_name标签。该脚本会提取text和screen_name标签对应的内容并仅显示出这两个字段

# 脚本将这个长字符串分配给变量TO_ret

# JSON格式使用引用字符串作为键，对应的值是否写成引用形式均可。键/值序列之间用逗号分隔，键与值之间用冒号分隔

# 第一个sed命令将"替换成换行符，使得每个键/值序列都出现在单独的一行中。这些行通过管道传入另一个sed命令，在这里将每一处":替换成波浪号（~），处理后的结果类似于这样：
screen_name~"Clif_Flynt" 

# 最后的awk脚本读取每一行。选项-F~使得awk在波浪号处将行分割成字段，因此$1中保存的是键，$2中保存的是值。if命令会检查text或screen_name。text在tweet中先出现，但是如果我们先输出推送人（sender）的话，会更容易读取。因此脚本保存text所对应的值，等碰到screen_name时，输出$2的值以及之前保存的text的值

# 库函数TO_statuses_update可用来发布新的tweet。如果该函数的第一个参数为空，则表明使用默认格式，要发布的内容可以作为函数的第二个参数
```

#### 通过Web服务器查询单词含义

网上有一些提供了API的词典，利用这些API可以在脚本中通过网站查询词汇。这则脚本展
示了如何使用其中一款流行的词典工具

我们打算使用curl、sed和grep来编写一个词汇查询工具。词典类网站数不胜数，你可以
注册并免费使用网站的API（限于个人用途）。在这里，我们使用Merriam-Webster的词典API。请按照下列步骤执行

(1) 进入http://www.dictionaryapi.com/register/index.htm注册账户。选择Collegiate Dictionary和
Learner’s Dictionary
(2) 使用新创建的用户登录，进入My Keys获取密钥。记下Learner’s Dictionary的密钥

```shell
# 下面这段脚本可以显示出词汇含义：

#!/bin/bash
# 文件名: define.sh
# 用途:用于从dictionaryapi.com获取词汇含义

key=YOUR_API_KEY_HERE

if [ $# -ne 2 ];
then
  echo -e "Usage: $0 WORD NUMBER"
  exit -1;
fi

curl --silent \
http://www.dictionaryapi.com/api/v1/references/learners/xml/$1?key=$key | \
	grep -o \<dt\>.*\</dt\> | \
	sed 's$</*[a-z]*>$$g' | \
	head -n $2 | nl 
	
# 运行脚本：
$ ./define.sh usb 1
1 :a system for connecting a computer to another device (such as
a printer, keyboard, or mouse) by using a special kind of cord a
USB cable/port USB is an abbreviation of "Universal Serial Bus."How
it works... 
```

工作原理

```shell
# 我们使用curl，通过指定API key（$apikey）以及待查找含义的词汇（$1）从词典API页面获取相关数据。包含定义的查询结果位于<dt>标签中，可以使用grep来将其选中。sed命令用于删除标签。脚本从词汇含义中提取所需要的行数并使用n1在行前加上行号
```

#### 查找网站中的无效链接

我们必须要检查网站中的无效链接。在大型网站上采用人工方式检查是不现实的。好在这种
活儿很容易实现自动化。我们可以利用HTTP处理工具来找出无效的链接

我们使用lynx和curl识别链接并找出其中的无效链接。lynx有一个-traversal选项，能够
以递归方式访问网站页面并建立所有超链接的列表。我们可以用curl验证每一个链接的有效性

```shell
# 下面的脚本利用lynx和curl查找Web页面上的无效链接：

#!/bin/bash
#文件名: find_broken.sh
#用途: 查找网站中的无效链接
if [ $# -ne 1 ];
then
  echo -e "$Usage: $0 URL\n"
  exit 1;
fi

echo Broken links:

mkdir /tmp/$$.lynx
cd /tmp/$$.lynx

lynx -traversal $1 > /dev/null
count=0;

sort -u reject.dat > links.txt

while read link;
do
  output=`curl -I $link -s \
| grep -e "HTTP/.*OK" -e "HTTP/.*200"`
  if [[ -z $output ]];
  then
    output=`curl -I $link -s | grep -e "HTTP/.*301"`
    if [[ -z $output ]];
      then
      echo "BROKEN: $link"
      let count++
    else
      echo "MOVED: $link"
    fi
  fi
done < links.txt

[ $count -eq 0 ] && echo No broken links found.
```

工作原理

```shell
# lynx -traversal URL会在当前工作目录下生成多个文件，其中包括reject.dat，该文件包含网站中的所有链接。sort -u用来建立一个不包含重复项的列表。然后，我们迭代每一个链接并通过curl -I检验接收到的响应头部。如果响应头部的第一行包含HTTP/以及OK或200，就表示该链接正常。如果链接不正常，进一步检查响应状态码是否为301（永久性转移）。如果仍不是，则将这个无效链接输出到屏幕
```

从名称上来看，reject.dat中包含的应该是无效URL的列表。但其实并非如此，
lynx是将所有的URL全都放到了这个文件中

lynx还生成了一个名为traverse.errors的文件，其中包含了所有在浏览过程中
存在问题的URL。但是lynx只会将返回HTTP 404(not found)的URL放入该
文件，因此会遗漏那些存在其他类型错误的URL（例如HTTP 403 Forbidden）。
这就是为什么要手动检查返回状态的原因

#### 跟踪网站变动

对于Web开发人员和用户来说，能够跟踪网站的变动情况是件好事，但靠人工检查就不实际
了。我们可以编写一个定期运行的变动跟踪脚本来完成这项任务。一旦发生变动，脚本便会发出
提醒

用bash脚本跟踪网站变动意味着要在不同的时间检索网站，然后用diff命令进行比对。我们
可以使用curl和diff来实现

```shell
# 下面的bash脚本结合了各种命令来跟踪页面变动：

#!/bin/bash
# 文件名: change_track.sh
# 用途: 跟踪页面变动

if [ $# -ne 1 ];
then
  echo -e "$Usage: $0 URL\n"
  exit 1;
fi

first_time=0
# 非首次运行

if [ ! -e "last.html" ];
then
  first_time=1
  # 首次运行
fi

curl --silent $1 -o recent.html

if [ $first_time -ne 1 ];
then
  changes=$(diff -u last.html recent.html)
  if [ -n "$changes" ];
  then
    echo -e "Changes:\n"
    echo "$changes"
  else
    echo -e "\nWebsite has no changes"
  fi
else
 echo "[First run] Archiving.."
fi

cp recent.html last.html 
```

让我们分别观察一下网页未发生变动和发生变动后脚本track_changes.sh的输出

注意把MyWebSite.org改成你自己的网站名

```shell
# 第一次运行：
$ ./track_changes.sh http:// www.MyWebSite.org
[First run] Archiving.. 

# 第二次运行：
$ ./track_changes.sh http://www.MyWebSite.org
Website has no changes 

# 在网页变动后，第三次运行：
$ ./track_changes.sh http://www.MyWebSite.org
Changes:
--- last.html 2010-08-01 07:29:15.000000000 +0200
+++ recent.html 2010-08-01 07:29:43.000000000 +0200
@@ -1,3 +1,4 @@
<html>
 +added line :)
 <p>data</p>
</html> 
```

工作原理

```shell
# 脚本用[ ! -e "last.html" ];检查自己是否是首次运行。如果last.html不存在，那就意味着这是首次运行，必须下载Web页面并将其复制为last.html

# 如果不是首次运行，那么脚本应该下载一个新的页面副本（recent.html），然后用diff检查差异。如果有变化，则打印出变更信息并将recent.html复制成last.html

#注意，网站会在作出修改的第一次检查时产生体积巨大的diff文件。如果要跟踪多个页面，你可以为每个网站分别创建相应的目录
```

#### 发送Web页面并读取响应

POST和GET是HTTP的两种请求类型，用于发送或检索信息。在GET请求方式中，我们利用
页面的URL来发送参数（名称-值）。而在POST请求方式中，参数是放在HTTP消息主体中发送的。
POST方式常用于提交内容较多的表单或是私密信息

这里我们使用了tclhttpd软件包中自带的样例网站guestbook。
你可以从http://sourceforge.net/projects/tclhttpd 下载tclhttpd，然后在本地系统上运行，创建一个本地Web服务器。如果用户点击按钮Add me to your guestbook，页面会发送一个包含姓名和URL的请求，请求中的信息会被添加到guestbook的页面上，以显示出都有谁访问过该站点

这个过程可以使用一条curl（或wget）命令实现自动化

```shell
# 下载tclhttpd软件包，切换到bin目录。启动tclhttpd守护进程：
tclsh httpd.tcl

# 使用curl发送POST请求并读取网站的响应（HTML格式）：
$ curl URL -d "postvar=postdata2&postvar2=postdata2"
# 例如：
$ curl http://127.0.0.1:8015/guestbook/newguest.html \
-d "name=Clif&url=www.noucorp.com&http=www.noucorp.com" 

# curl会打印出响应页面：
<HTML>
<Head>
<title>Guestbook Registration Confirmed</title>
</Head>
<Body BGCOLOR=white TEXT=black>
<a href="www.noucorp.com">www.noucorp.com</a>

<DL>
<DT>Name
<DD>Clif
<DT>URL
<DD>
</DL>
www.noucorp.com

</Body>

# -d表示以POST方式提交用户数据
# -d的字符串参数形式类似于GET请求
# 每对var=value之间用&分隔

# 也可以利用wget的--post-data "string"来提交数据。例如：
$ wget http://127.0.0.1:8015/guestbook/newguest.cgi \
--post-data "name=Clif&url=www.noucorp.com&http=www.noucorp.com" \
-O output.html 

# “名称-值”的格式同cURL中一样。output.html中的内容和cURL命令返回的一样

<form action="newguest.cgi" " method="post" >
<ul>
<li> Name: <input type="text" name="name" size="40" >
<li> Url: <input type="text" name="url" size="40" >
<input type="submit" >
</ul>
</form>

# 其中，newguest.cgi是目标URL。当用户输入详细信息并点击Submit按钮时，姓名和URL就以POST请求的方式被发送到newguest.cgi页面，然后响应页面被返回到浏览器
```

以POST形式发送的字符串（例如-d或--post-date）总是应该以引用的形
式给出。否则，&会被shell解读为该命令需要作为后台进程运行

#### 从Internet下载视频

下载视频的原因有很多。如果你使用的是计量服务（metered service），可能想要在资费较低
的闲暇时段下载视频。也可能是因为网络带宽不足以支持流媒体，亦或是想永久保留一份可爱的
喵星人的视频秀给好朋友们看

有一个叫作youtube-dl的视频下载工具。多数发行版中并没有包含这个工具，软件仓库里
的版本也未必是最新的，因此最好是去官方网站下载（http://yt-dl.org）

按照页面上的链接和信息下载并安装youtube-dl

```shell
# youtube-dl用起来很简单。打开浏览器，找到你喜欢的视频。将视频的URL复制/粘贴到youtube-dl的命令行中：
youtube-dl https://www.youtube.com/watch?v=AJrsl3fHQ74
# 下载完成之后，youtube-dl会在终端中生成一条状态信息
```

工作原理

```shell
# youtube-dl通过向服务器发出GET请求（就像浏览器一样）来实现视频下载。它会伪装成浏览器，使得YouTube或其他视频提供商以为这是一台流媒体设备，从而下载到视频

# 选项-list-formats（-F）会列出支持的视频格式，选项-format（-f）可以指定下载哪种格式的视频。如果你的Internet连接带宽不足，而你又想下载高分辨率视频的时候，这个选项就用得上了
```

#### 使用OTS汇总文本

开放文本摘要器（Open Text Summarizer，OTS）可以从文本中删除无关紧要的内容，生成
一份简洁的摘要

大多数Linux发行版并不包含ots软件包，可以通过下列命令进行安装：

```shell
apt-get install libots-devel 
```

```shell
# ots用起来很简单。它从文件或stdin中读取输入，将生成的摘要输出到stdout：
ots LongFile.txt | less
# 或者
cat LongFile.txt | ots | less 
# ots也可以结合curl生成网站的摘要信息。例如，你可以用ots为那些絮絮叨叨的博客做摘要：
curl http://BlogSite.org | sed -r 's/<[^>]+>//g' | ots | less 
```

工作原理

curl命令从博客站点中检索页面并将其传给sed。sed命令利用正则表达式删除所有的
HTML标签和分别以小于号和大于号作为起止的字符串。余下的文本被传入ots，后者生成的摘
要信息由less命令显示出来

#### 在命令行中翻译文本

你可以通过浏览器访问Google所提供的在线翻译服务。Andrei Neculau编写了一个awk脚本，可以从命令行中访问该服务并进行翻译

大多数Linux发行版中都没有包含这个命令行翻译器，不过你可以从Git直接安装：

```shell
cd ~/bin
wget git.io/trans
chmod 755 ./trans 
```

```shell
# trans可以将文本翻译成locale环境变量所设置的语言：
$> trans "J'adore Linux"
J'adore Linux
I love Linux
Translations of J'adore Linux
French -> English
J'adore Linux
I love Linux

# 你可以在待翻译的文本前使用选项来控制翻译所用的语言。选项格式如下：
from:to

# 要想将英语翻译成法语，可以使用下列命令：
$> trans en:fr "I love Linux"
J'aime Linux 
```

trans程序包含了5000行左右的awk代码，其中使用了curl来获取Google、Bing以及Yandex
的翻译服务

