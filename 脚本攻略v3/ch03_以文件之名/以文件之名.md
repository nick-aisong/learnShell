以文件之名
========
| 目录                           | 主要命令             |
| ------------------------------ | -------------------- |
| 生成任意大小的文件 |                   |
| 文本文件的交集与差集 |                   |
| 查找并删除重复文件 |                   |
| 文件权限、所有权与粘滞位 |                   |
| 将文件设置为不可修改 |                   |
| 批量生成空白文件 |                   |
| 查找符号链接及其指向目标 |                   |
| 枚举文件类型统计信息 |                   |
| 使用环回文件 |                   |
| 生成ISO及混合型ISO文件 |                   |
| 查找并修补文件差异 |          |
| 使用head与tail打印文件的前10行和后10行 |                   |
| 只列出目录的各种方法 |                   |
| 在命令行中使用pushd和popd实现快速定位 |                   |
| 统计文件的行数、单词数和字符数 |                   |
| 打印目录树 |                   |
| 处理视频与图像文件 |                   |

#### 生成任意大小的文件

包含随机数据的文件可用于测试。你可以使用这种文件测试应用程序效率，确定应用程序没
有输入方面的缺陷和大小方面的限制，创建环回文件系统（环回文件自身包含文件系统，这种文
件可以像物理设备一样使用mount命令进行挂载）等。Linux提供了一些可用于构建此类文件的实
用工具

创建特定大小的文件最简单的方法就是利用dd命令。dd命令会克隆给定的输入内容，然后
将一模一样的一份副本写入到输出。stdin、设备文件、普通文件等都可作为输入，stdout、
设备文件、普通文件等也可作为输出

```shell
# 下面是使用dd命令的一个示例：

$ dd if=/dev/zero of=junk.data bs=1M count=1
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.00767266 s, 137 MB/s 
# 该命令会创建一个内容全部为零的1MB大小的文件junk.data
```

来看一下命令参数：

- if表示输入文件（input file）
- of表示输出文件（output file）
- bs指定了以字节为单位的块大小（block size）
- count表示需要被复制的块数

注意：以root身份使用dd命令时一定得留意，该命令运行在设备底层。要是你不小
心出了岔子，搞不好会把磁盘清空或是损坏数据。一定要反复检查dd命令所用
的语法是否正确，尤其是参数of=

块大小（bs）可以使用各种计量单位

| 单元大小         | 代 码 |
| ---------------- | ----- |
| 字节（1B）       | C     |
| 字（2B）         | w     |
| 块（512B）       | B     |
| 千字节（1024B）  | K     |
| 兆字节（1024KB） | M     |
| 吉字节（1024MB） | G     |

我们可以利用bs来生成任意大小的文件。除了MB，表中给出的其他计量单位都可以使用

/dev/zero是一个特殊的字符设备，它会返回0值字节（\0）

如果不指定输入参数（if），dd会从stdin中读取输入。如果不指定输出参数（of），则dd
会使用stdout作为输出

使用dd命令也能够用来测量内存操作的速度，这可以通过向/dev/null传输大量数据并观
察命令输出来实现（例如，在前一个例子中显示出的1048576 bytes (1.0 MB) copied,
0.00767266 s, 137 MB/s）


#### 文本文件的交集与差集

交集（intersection）和差集（set difference）操作在数学课上的集合论中经常会被用到。有时
候，也需要对字符串执行类似的操作

comm命令可用于比较两个已排序的文件。它可以显示出第一个文件和第二个文件所独有的
行以及这两个文件所共有的行。该命令有一些选项可以禁止显示指定的列，以便于执行交集和求
差操作

- 交集（intersection）：打印出两个文件所共有的行
- 求差（difference）：打印出指定文件中所包含的互不相同的那些行
- 差集（set difference）：打印出包含在文件A中，但不包含在其他指定文件（例如B和C）中的那些行

```shell
# 需要注意的是comm必须使用两个排过序的文件作为输入。下面是我们用到的输入文件：
$ cat A.txt
apple
orange
gold
silver 
steel
iron

$ cat B.txt
orange
gold
cookies
carrot

$ sort A.txt -o A.txt ; sort B.txt -o B.txt 

# 1. 首先执行不带任何选项的comm
$ comm A.txt B.txt
apple
		carrot
		cookies
				gold
iron
				orange
silver
steel
# 输出的第一列包含只在A.txt中出现的行，第二列包含只在B.txt中出现的行，第三列包含A.txt和B.txt中共有的行
# 各列之间以制表符（\t）作为分隔符

# 2. 为了打印两个文件的交集，我们需要删除前两列，只打印出第三列
# -1选项可以删除第一列，-2选项可以删除第二列，最后留下的就是第三列
$ comm A.txt B.txt -1 -2
gold
orange 

# 3. 删除第三列，就可以打印出两个文件中互不相同的那些行
apple
		carrot
		cookies
iron
silver
steel

# 输出中包含着夹杂有空白的两列，显示了在file1和file2中存在的唯一的行
# 要想提高输出结果的可用性，可以将两列合并成一列，就像这样
apple
carrot
cookies
iron
silver
steel

# 4. 可以使用tr删除制表符来合并两列
$ comm A.txt B.txt -3 | tr -d '\t'
apple
carrot
cookies
iron
silver
steel 

# 5. 通过删除不需要的列，我们就可以分别得到A.txt和B.txt的差集
# A.txt的差集
$ comm A.txt B.txt -2 -3 

# B.txt的差集
$ comm A.txt B.txt -1 -3 
```

comm命令还接受字符-作为命令行参数，借此实现从stdin中读取输入。这就提供了一种比
较多个文件的方法

```shell
# 假设我们有一个文件C.txt：
$> cat C.txt
pear
orange
silver
mithral 

# 我们可以将文件B.txt和C.txt与A.txt相比较：
$> sort B.txt C.txt | comm - A.txt
	apple
carrot
cookies
		gold
	iron
mithral
		orange
pear
		silver
	steel 
```

#### 查找并删除重复文件

我们可以通过比较文件内容来识别重复文件。校验和是一种理想的解决方法。内容相同的文
件自然会生成相同的校验和

下面是查找并删除重复文件的步骤

```shell
# 1. 创建一些测试文件
$ echo "hello" > test ; cp test test_copy1 ; cp test test_copy2;
$ echo "next" > other;
# test_copy1和test_copy2都是test文件的副本

# 2. 我们在脚本中使用awk来删除重复文件

#!/bin/bash
# 文件名: remove_duplicates.sh
# 用途: 查找并删除重复文件，每一个文件只保留一份
ls -lS --time-style=long-iso | awk 'BEGIN {
  getline; getline;
  name1=$8; size=$5
}
{
  name2=$8;
  if ( size==$5 )
  {
    "md5sum "name1 | getline; csum1=$1;
    "md5sum "name2 | getline; csum2=$1;
    if ( csum1==csum2 )
    {
      print name1; print name2
    }
  };

size=$5; name1=name2;
}' | sort -u > duplicate_files

cat duplicate_files | xargs -I {} md5sum {} | \
sort | uniq -w 32 | awk '{ print $2 }' | \
sort -u > unique_files

echo Removing..
comm duplicate_files unique_files -3 | tee /dev/stderr | xargs rm
echo Removed duplicates files successfully. 
 
 # 3. 执行该脚本
 $ ./remove_duplicates.sh
```

工作原理

ls -lS对当前目录下的所有文件按照文件大小进行排序并列出文件的详细信息。
--time-style=long-iso告诉ls依照ISO格式打印日期。awk读取ls -lS的输出，对行列进行
比较，找出重复文件

这段代码的执行逻辑如下
- 我们将文件依据大小排序并列出，这样大小相同的文件就会排列在一起。识别大小相同
  的文件是我们查找重复文件的第一步。接下来，计算这些文件的校验和。如果校验和相
  同，那么这些文件就是重复文件，将被删除

- 在进行主要处理之前，首先要执行awk的BEGIN{}语句块。该语句块读取文件所有的行并
  初始化变量。处理ls剩余的输出都是在{}语句块中完成的。读取并处理完所有的行之后，
  执行END{}语句块。ls -lS的输出如下：

  ```shell
  total 16
  -rw-r--r-- 1 slynux slynux 5 2010-06-29 11:50 other
  -rw-r--r-- 1 slynux slynux 6 2010-06-29 11:50 test
  -rw-r--r-- 1 slynux slynux 6 2010-06-29 11:50 test_copy1
  -rw-r--r-- 1 slynux slynux 6 2010-06-29 11:50 test_copy2 
  ```
- 第1行输出告诉了我们文件的总数量，这个信息在本例中没什么用处。我们用getline读
取该行，然后丢弃掉。我们需要比对每一行及其下一行的文件大小。在BEGIN语句块中，
使用getline读取文件列表的第一行并存储文件名和大小分别对应第8列和第5列）。当
awk进入{}语句块后，依次读取余下的行（一次一行）。在该语句块中，将从当前行中得
到的文件大小与之前存储在变量size中的值进行比较。如果相等，那就意味着两个文件
至少在大小上是相同的，必须再用md5sum做进一步的检查

我们在给出的解决方法中使用了一些技巧

```shell
# 在awk内部可以读取外部命令的输出：
"cmd"| getline 
```

读入一行后，该行就被保存在$0中，行中的每一列分别被保存在$1、$2…$n中。我们将文
件的md5校验和分别保存在变量csum1和csum2中。变量name1和name2保存文件列表中相邻两
个文件的文件名。如果两个文件的校验和相同，那它们肯定是重复文件，其文件名会被打印出来

我们需要从每组重复文件中找出一个文件，这样就可以删除其他副本了。计算重复文件的
md5sum值并从每一组重复文件中打印出其中一个。这是通过用-w 32比较每一行的md5sum输出
中的前32个字符（md5sum的输出通常由32个字符的散列值和文件名组成）来找出那些不相同的
行（注：也就是不重复的文件）。这样，每组重复文件中的一个采样就被写入unique_files文件

现在需要将duplicate_files中列出的、未包含在unique_files之内的文件全部删除。comm命令可以将其打印出来

对此，我们可以使用差集操作来实现

comm只能处理排序过的文件。因此，使用sort -u来过滤duplicate_files和unique_files文件

tee可以将文件名传给rm命令并输出。tee可以将输出发送至stdout和另一个文件中。我
们也可以将文本重定向到stderr来实现终端打印功能。/dev/stderr是对应于stderr（标准错误）
的设备。通过重定向到stderr设备文件，发送到stdin的文本将会以标准错误的形式出现在终
端中

#### 文件权限、所有权与粘滞位

文件权限和所有权是Unix/Linux文件系统的显著特性之一。这些特性能够在多用户环境中保
护你的个人信息。不匹配的权限和所有权也会导致文件共享方面的难题。这则攻略讲解了如何有
效地设置文件的权限和所有权

每一个文件都拥有多种类型的权限。在这些权限中，我们通常要和三组权限打交道（用户、
用户组以及其他用户）

用户（user）是文件的所有者，通常拥有所有的访问权。用户组（group）是多个用户的集合
（由系统管理员指定），可能拥有文件的部分访问权。其他用户（others）是除文件所有者或用户
组成员之外的任何人

```shell
# ls命令的-l选项可以显示出包括文件类型、权限、所有者以及组在内的多方面信息：
-rw-r--r-- 1 slynux users 2497 2010-02-28 11:22 bot.py
drwxr-xr-x 2 slynux users 4096 2010-05-27 14:31 a.py
-rw-r--r-- 1 slynux users 539 2010-02-10 09:11 cl.pl 
```

第1列表明了文件类型

- -：普通文件
- d：目录
- c：字符设备
- b：块设备
- l：符号链接
- s：套接字
- p：管道


接下来的9个字符可以划分成三组，每组3个字符（--- --- ---）。第一组的3个字符对应
用户权限（所有者），第二组对应用户组权限，第三组对应其他用户权限。这9个字符（即9个权
限）中的每一个字符指明是否其设置了某种权限。如果已设置，对应位置上会出现一个字符，否
则出现一个-，表明没有设置对应的权限

有3种常见的字符

- r（read）：如果设置，表明该文件、设备或目录可读

- w（write）：如果设置，表明该文件、设备或目录可以被修改。对于目录而言，此权限指
定了是否可以在目录下创建或删除文件

- x（execute）：如果设置，表明该文件可执行。对于目录而言，此权限指定了能否访问
目录下的文件

让我们来看一下每组权限对于用户、用户组以及其他用户的含义

- 用户（权限序列：rwx------）：定义了用户权限。通常来说，对于数据文件，用户权限
是rw-；对于脚本或可执行文件，用户权限是rwx。用户还有一个称为setuid（S）的特
殊权限，它出现在执行权限（x）的位置。setuid权限允许可执行文件以其拥有者的权
限来执行，即使这个可执行文件是由其他用户运行的。具有setuid权限文件的权限序列
可以是这样的：-rwS------

- 用户组（权限序列：---rwx---）：第二组字符指定了组权限。组权限中并没有setuid，
但是有一个setgid（S）位。它允许使用与可执行文件所属组权限相同的有效组来运行
该文件。但是这个组和实际发起命令的用户组未必相同。例如，组权限的权限序列可以
是这样的：----rwS---

- 其他用户（权限序列：------rwx）：最后3个字符是其他用户权限。如果设置了相应的
权限，其他用户也可以访问特定的文件或目录。作为一种规则，通常将这组权限设置为---

目录有一个叫作粘滞位（sticky bit）的特殊权限。如果目录设置了粘滞位，只有创建该目录
的用户才能删除目录中的文件，就算用户组和其他用户也有写权限，仍无能无力。粘滞位出现在
其他用户权限组中的执行权限（x）位置。它使用t或T来表示。如果没有设置执行权限，但设置
了粘滞位，就使用T；如果同时设置了执行权限和粘滞位，就使用t

例如：

```shell
------rwt , ------rwT 
```







#### 将文件设置为不可修改








#### 批量生成空白文件








#### 查找符号链接及其指向目标









#### 枚举文件类型统计信息









#### 使用环回文件









#### 生成ISO及混合型ISO文件









#### 查找并修补文件差异









#### 使用head与tail打印文件的前10行和后10行









#### 只列出目录的各种方法









#### 在命令行中使用pushd和popd实现快速定位









#### 统计文件的行数、单词数和字符数









#### 打印目录树









#### 处理视频与图像文件














































































